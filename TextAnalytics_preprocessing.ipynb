{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/chinwen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/chinwen/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to /Users/chinwen/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/chinwen/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize \n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords, words\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chinwen/anaconda/envs/AML/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (43,61,62,88) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "listingsAll = pd.read_csv(\"listingsAll.csv\")\n",
    "reviewsAll = pd.read_csv(\"reviewsAll.csv\")\n",
    "cleaned = pd.read_csv(\"cleaned_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_cat(x):\n",
    "    if x >= 90:\n",
    "        return 'A'\n",
    "    elif (x < 90) & (x >= 75):\n",
    "        return 'B'\n",
    "    else:\n",
    "        return 'C'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. add rating category to the cleaned file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_categ = listingsAll[['id', 'summary', 'review_scores_rating']].copy()\n",
    "summary_categ['rating_categ'] = summary_categ['review_scores_rating'].map(lambda x: score_cat(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedPlus = cleaned.copy()\n",
    "cleanedPlus['rating_categ'] = summary_categ['rating_categ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedPlus.to_csv(\"CleanedPlus.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary to map listings id to rating category\n",
    "rating_categ = {}\n",
    "_ = summary_categ.apply(lambda x: rating_categ.update({x.id: x.rating_categ}), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. adjective words in summary for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 39s, sys: 519 ms, total: 1min 39s\n",
      "Wall time: 1min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# rating score >= 90 (i.e. A)\n",
    "high_words_list = [word_tokenize(str(e)) for e in summary_categ['summary'][summary_categ['rating_categ'] == 'A']]\n",
    "high_words = [w for e in high_words_list for w in e]\n",
    "high_pos = nltk.pos_tag(high_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_adj = []\n",
    "for p in high_pos:\n",
    "    if p[1].startswith('JJ'):\n",
    "        high_adj.append(p[0].lower())\n",
    "        \n",
    "high = sorted(dict(Counter(high_adj)).items(), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.1 s, sys: 68.3 ms, total: 18.2 s\n",
      "Wall time: 18.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 90 > rating score >= 75 (i.e. B)\n",
    "mid_words_list = [word_tokenize(str(e)) for e in summary_categ['summary'][summary_categ['rating_categ'] == 'B']]\n",
    "mid_words = [w for e in mid_words_list for w in e]\n",
    "mid_pos = nltk.pos_tag(mid_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_adj = []\n",
    "for p in mid_pos:\n",
    "    if p[1].startswith('JJ'):\n",
    "        mid_adj.append(p[0].lower())\n",
    "        \n",
    "mid = sorted(dict(Counter(mid_adj)).items(), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37.1 s, sys: 73.8 ms, total: 37.2 s\n",
      "Wall time: 37.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# rating score < 75 (i.e. C)\n",
    "low_words_list = [word_tokenize(str(e)) for e in summary_categ['summary'][summary_categ['rating_categ'] == 'C']]\n",
    "low_words = [w for e in low_words_list for w in e]\n",
    "low_pos = nltk.pos_tag(low_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_adj = []\n",
    "for p in low_pos:\n",
    "    if p[1].startswith('JJ'):\n",
    "        low_adj.append(p[0].lower())\n",
    "        \n",
    "low = sorted(dict(Counter(low_adj)).items(), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('adj_summary.csv', 'w')\n",
    "f.write(\"word#freq#category\\n\")\n",
    "for l in high:\n",
    "    f.write(str(l[0]) + \"#\" + str(l[1]) + \"#\" + 'A' + \"\\n\")\n",
    "for l in mid:\n",
    "    f.write(str(l[0]) + \"#\" + str(l[1]) + \"#\" + 'B' + \"\\n\")\n",
    "for l in low:\n",
    "    f.write(str(l[0]) + \"#\" + str(l[1]) + \"#\" + 'C' + \"\\n\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. adjective words in review for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = reviewsAll.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "r['comments'] = r['comments'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 3s, sys: 2.97 s, total: 9min 6s\n",
      "Wall time: 9min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "r['words'] = r['comments'].map(lambda x: word_tokenize(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopwords(row):\n",
    "    clean_words = []\n",
    "    for w in row:\n",
    "        if w not in stopWords:\n",
    "            clean_words.append(w)\n",
    "    return clean_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "r['words'] = r['words'].map(lambda x: removeStopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31min 21s, sys: 10.1 s, total: 31min 31s\n",
      "Wall time: 31min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "r['pos_tagger']  = r['words'].map(lambda x:pos_tag(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_adj(ps):\n",
    "    adj = []\n",
    "    for p in ps:\n",
    "        if p[1].startswith('JJ'):\n",
    "            adj.append(p[0].lower()) \n",
    "    return adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_noun(ps):\n",
    "    noun = []\n",
    "    for p in ps:\n",
    "        if p[1].startswith('NN'):\n",
    "            noun.append(p[0].lower()) \n",
    "    return noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.5 s, sys: 114 ms, total: 12.6 s\n",
      "Wall time: 12.6 s\n"
     ]
    }
   ],
   "source": [
    "r['adj'] = r['pos_tagger'].map(lambda x: find_adj(x))\n",
    "r['number_adj'] = r['adj'].map(lambda x: len(x))\n",
    "r['noun'] = r['pos_tagger'].map(lambda x: find_noun(x))\n",
    "r['number_noun'] = r['noun'].map(lambda x: len(x))\n",
    "r['number_words'] = r['words'].map(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.to_csv('reviewsAllPlus.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = pd.read_csv(\"reviewsAllPlus.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_short = r.drop(['words', 'pos_tagger', 'adj', 'noun', 'Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_short.to_csv('reviewsPlus.csv', index=False, na_rep = 'NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary to map ratings category to number of adjective words\n",
    "adj_r_d = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_adj_dict(adj, listing_id):\n",
    "    if rating_categ[listing_id] not in adj_r_d:\n",
    "        adj_r_d[rating_categ[listing_id]] = {}\n",
    "    for word in adj:\n",
    "        word = word.lower() \n",
    "        if word not in adj_r_d[rating_categ[listing_id]]:\n",
    "            adj_r_d[rating_categ[listing_id]][word] = 0\n",
    "        adj_r_d[rating_categ[listing_id]][word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = r.apply(lambda x: add_to_adj_dict(x.adj, x.listing_id), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 54s, sys: 12.3 s, total: 2min 6s\n",
      "Wall time: 2min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#remove non-English words\n",
    "for k in adj_r_d:\n",
    "    for w in (list(adj_r_d[k].keys())[:300]):\n",
    "        if w not in set(words.words()):\n",
    "            adj_r_d[k].pop(w, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_remove = ['un', 'nous', 'es', 'está', 'u', 'las', 'se', 'lo', 'ne', 'el', 'tal', 'den', \n",
    "                  'sehr', 'en', 'la', 'een', \"'\", 'que']\n",
    "for k in adj_r_d:\n",
    "    for w in list(adj_r_d[k].keys())[:300]:\n",
    "        if w in words_to_remove:\n",
    "            adj_r_d[k].pop(w, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr = sorted(adj_r_d['A'].items(), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr = sorted(adj_r_d['B'].items(), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = sorted(adj_r_d['C'].items(), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('adj_review.csv', 'w')\n",
    "f.write(\"word#freq#category\\n\")\n",
    "for l in hr:\n",
    "    f.write(str(l[0]) + \"#\" + str(l[1]) + \"#\" + 'A' + \"\\n\")\n",
    "for l in mr:\n",
    "    f.write(str(l[0]) + \"#\" + str(l[1]) + \"#\" + 'B' + \"\\n\")\n",
    "for l in lr:\n",
    "    f.write(str(l[0]) + \"#\" + str(l[1]) + \"#\" + 'C' + \"\\n\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. amenities word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cleaned[['amenities']].copy()\n",
    "df['cleaned_amentities'] = df['amenities'].apply(lambda x: x.replace('{','')\n",
    "                                                 .replace('}','').replace('\\\"','').replace('/',' ')\n",
    "                                                 .replace('’','').lower().split(','))\n",
    "w = [w for e in df['cleaned_amentities'] for w in e]\n",
    "c = sorted(dict(Counter(w)).items(), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('amenities.csv', 'w')\n",
    "f.write(\"word#freq\\n\")\n",
    "for l in c:\n",
    "    f.write(str(l[0]) +\"#\"+ str(l[1]) + \"\\n\")\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:AML]",
   "language": "python",
   "name": "conda-env-AML-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
